# 复制时延

## max_standby_streaming_delay、max_standby_archive_delay

## 参数含义

从节点最长流复制/归档时间，默认30s，超过该值的查询会被取消。

设置 <0 的值代表永不超时，这样，从节点有查询，会使得startup进程一直处于wait状态。

## 复现问题步骤

主备节点参数置为-1

max_standby_archive_delay = -1

max_standby_streaming_delay = -1

修改后执行pg_ctl reload -D $PGDATA

1、主节点建表，插入一行

```none
postgres=# create table t1(id int);
CREATE TABLE
postgres=# insert into t1 values(1);
INSERT 0 1
```

2、备节点开启查询

```none
postgres=# begin;
BEGIN
postgres=# select * from t1;
 id
----
  1
(1 row)
-- 不要退出事务.
```

3、主节点删除表t1

```none
postgres=# drop table t1;
DROP TABLE
```

删除t1表后, 这部分xlog信息在standby上面做恢复时将和standby上面的事务冲突.

4、  查看standby的replay_location, 这个指的是恢复点.  

```none
postgres=# select * from pg_stat_replication ;
-[ RECORD 1 ]----+------------------------------
pid              | 7312
usesysid         | 17063
usename          | replica
application_name | walreceiver
client_addr      | 192.168.31.131
client_hostname  |
client_port      | 50977
backend_start    | 2019-08-23 21:53:31.316891-04
backend_xmin     | 10237
state            | streaming
sent_location    | 2/4EDCD178
write_location   | 2/4EDCD178
flush_location   | 2/4EDCD178
replay_location  | 2/4EDC9888
sync_priority    | 1
sync_state       | sync
```

参数释义：

sent_location    | 3/CF123560 # Master传送WAL的位置
　　
write_location   | 3/CF123560 # Slave接收WAL的位置
　　
flush_location   | 3/CF123560 # Slave同步到磁盘的WAL位置
　　
replay_location  | 3/CF123560 # Slave同步到数据库的WAL位置



5、查看从节点startup进程状态以及调用栈

[postgres@node2 ~]$ ps -ef|grep postgres|grep startup|grep -v grep
postgres   7370   7362  0 21:52 ?        00:00:00 postgres: startup process   recovering 00000006000000020000004E ***waiting waiting***

[postgres@node2 ~]$ pstack 7370
#0  0x00007f68b5dc4f53 in __select_nocancel () from /lib64/libc.so.6
#1  0x00000000009d61f6 in pg_usleep (microsec=1000000) at pgsleep.c:56
#2  0x00000000007fed49 in ***WaitExceedsMaxStandbyDelay*** () at standby.c:200
#3  0x00000000007fee85 in ResolveRecoveryConflictWithVirtualXIDs (waitlist=0x13062c8, reason=PROCSIG_RECOVERY_CONFLICT_LOCK) at standby.c:261
#4  0x00000000007ff07b in ResolveRecoveryConflictWithLock (locktag=...) at standby.c:404
#5  0x000000000080c4cc in ProcSleep (locallock=0x1300300, lockMethodTable=0xb5e120 <default_lockmethod>) at proc.c:1223
#6  0x00000000008056ed in WaitOnLock (locallock=0x1300300, owner=0x0) at lock.c:1745
#7  0x00000000008043b8 in LockAcquireExtended (locktag=0x7ffc44297930, lockmode=8, sessionLock=1 '\001', dontWait=0 '\000', reportMemoryError=1 '\001', locallockp=0x0) at lock.c:1026
#8  0x000000000080395d in LockAcquire (locktag=0x7ffc44297930, lockmode=8, sessionLock=1 '\001', dontWait=0 '\000') at lock.c:689
#9  0x00000000007ff40c in StandbyAcquireAccessExclusiveLock (xid=10237, dbOid=13325, relOid=33506) at standby.c:667
#10 0x00000000007ff8a7 in standby_redo (record=0x1302950) at standby.c:822
#11 0x00000000005339b0 in StartupXLOG () at xlog.c:6989
#12 0x00000000007999ae in StartupProcessMain () at startup.c:210
#13 0x0000000000547528 in AuxiliaryProcessMain (argc=2, argv=0x7ffc44298530) at bootstrap.c:419
#14 0x000000000079896a in StartChildProcess (type=StartupProcess) at postmaster.c:5306
#15 0x00000000007934ec in PostmasterMain (argc=1, argv=0x12d6a40) at postmaster.c:1322
#16 0x00000000006daeae in main (argc=1, argv=0x12d6a40) at main.c:228

发现其在一直在等从节点上的那条语句执行完，取消该语句，等待立马消失。

[postgres@node2 ~]$ ps -ef|grep postgres|grep startup|grep -v grep
postgres   7370   7362  0 21:52 ?        00:00:00 postgres: startup process   recovering 00000006000000020000004E

[postgres@node2 ~]$ pstack 7370
#0  0x00007f68b5dce463 in __epoll_wait_nocancel () from /lib64/libc.so.6
#1  0x00000000007f507c in WaitEventSetWaitBlock (set=0x1312240, cur_timeout=5000, occurred_events=0x7ffc442976a0, nevents=1) at latch.c:1053
#2  0x00000000007f4f57 in WaitEventSetWait (set=0x1312240, timeout=5000, occurred_events=0x7ffc442976a0, nevents=1) at latch.c:1007
#3  0x00000000007f46e6 in WaitLatchOrSocket (latch=0x7f68aa8fd660, wakeEvents=25, sock=-1, timeout=5000) at latch.c:395
#4  0x00000000007f45d7 in WaitLatch (latch=0x7f68aa8fd660, wakeEvents=25, timeout=5000) at latch.c:350
#5  0x000000000053b9bc in WaitForWALToBecomeAvailable (RecPtr=9913029232, randAccess=0 '\000', fetching_ckpt=0 '\000', tliRecPtr=9913029208) at xlog.c:11789
#6  0x000000000053b067 in XLogPageRead (xlogreader=0x1302950, targetPagePtr=9913024512, reqLen=4720, targetRecPtr=9913029208, readBuf=0x1303968 "\223\320\005", readTLI=0x13031fc) at xlog.c:11301
#7  0x0000000000540c23 in ReadPageInternal (state=0x1302950, pageptr=9913024512, reqLen=4720) at xlogreader.c:574
#8  0x00000000005403f5 in XLogReadRecord (state=0x1302950, RecPtr=9913029208, errormsg=0x7ffc44297988) at xlogreader.c:276
#9  0x000000000052d57f in ReadRecord (xlogreader=0x1302950, RecPtr=0, emode=15, fetching_ckpt=0 '\000') at xlog.c:4045
#10 0x0000000000533ac9 in StartupXLOG () at xlog.c:7046
#11 0x00000000007999ae in StartupProcessMain () at startup.c:210
#12 0x0000000000547528 in AuxiliaryProcessMain (argc=2, argv=0x7ffc44298530) at bootstrap.c:419
#13 0x000000000079896a in StartChildProcess (type=StartupProcess) at postmaster.c:5306
#14 0x00000000007934ec in PostmasterMain (argc=1, argv=0x12d6a40) at postmaster.c:1322
#15 0x00000000006daeae in main (argc=1, argv=0x12d6a40) at main.c:228

由此可见，该select语句导致standby_redo 等待。

#### 相关函数：

用于判断时间是否超过max_standby_streaming_delay，如果到了，则返回true，否则返回false。

```
/*
 \* Standby wait logic for ResolveRecoveryConflictWithVirtualXIDs.
 \* We wait here for a while then return. If we decide we can't wait any
 \* more then we return true, if we can wait some more return false.
 */
static bool WaitExceedsMaxStandbyDelay(void)
{
​    TimestampTz ltime;
​    CHECK_FOR_INTERRUPTS();
    /* Are we past the limit time? */
​    ltime = GetStandbyLimitTime();
​    if (ltime && GetCurrentTimestamp() >= ltime)
​        return true;
​    /*
​     \* Sleep a bit (this is essential to avoid busy-waiting).
​     */
​    pg_usleep(standbyWait_us);
​    /*
​     \* Progressively increase the sleep times, but not to more than 1s, since
​     \* pg_usleep isn't interruptable on some platforms.
​     */
​    standbyWait_us *= 2;
​    if (standbyWait_us > 1000000)
​        standbyWait_us = 1000000;
​    return false;
}
```



解决查询冲突，standby节点上查询运行时间超过参数max_standby_streaming_delay时间限制的，取消会话。

```
static void ResolveRecoveryConflictWithVirtualXIDs(VirtualTransactionId *waitlist,
​                                       ProcSignalReason reason)
{
​    TimestampTz waitStart;
​    char       *new_status;
​    /* Fast exit, to avoid a kernel call if there's no work to be done. */
​    if (!VirtualTransactionIdIsValid(*waitlist))
​        return;
​    waitStart = GetCurrentTimestamp();
​    new_status = NULL;          /* we haven't changed the ps display */
​    while (VirtualTransactionIdIsValid(*waitlist))
​    {
​        /* reset standbyWait_us for each xact we wait for */
​        standbyWait_us = STANDBY_INITIAL_WAIT_US;
​        /* wait until the virtual xid is gone */
​        while (!VirtualXactLock(*waitlist, false))
​        {
​            /*
​             \* Report via ps if we have been waiting for more than 500 msec
​             \* (should that be configurable?)
​             */
​            if (update_process_title && new_status == NULL &&
​                TimestampDifferenceExceeds(waitStart, GetCurrentTimestamp(),
​                                           500))
​            {
​                const char *old_status;
​                int         len;
​                old_status = get_ps_display(&len);
​                new_status = (char *) palloc(len + 8 + 1);
​                memcpy(new_status, old_status, len);
​                strcpy(new_status + len, " waiting");
​                set_ps_display(new_status, false);
​                new_status[len] = '\0'; /* truncate off " waiting" */
​            }
​            /* Is it time to kill it? */
​            if (WaitExceedsMaxStandbyDelay())
​            {
​                pid_t       pid;
​                /*
​                 \* Now find out who to throw out of the balloon.
​                 */
​                Assert(VirtualTransactionIdIsValid(*waitlist));
​                pid = CancelVirtualTransaction(*waitlist, reason);
​                /*
​                 \* Wait a little bit for it to die so that we avoid flooding
​                 \* an unresponsive backend when system is heavily loaded.
​                 */
​                if (pid != 0)
​                    pg_usleep(5000L);
​            }
​        }
​        /* The virtual transaction is gone now, wait for the next one */
​        waitlist++;
​    }
​    /* Reset ps display if we changed it */
​    if (new_status)
​    {
​        set_ps_display(new_status, false);
​        pfree(new_status);
​    }
}
```



案例2 主库truncate t1，备库select t1发生冲突

备库相关配置

**hot_standby** = on            # "on" allows queries during recovery   (change requires restart)
**max_standby_archive_delay** = -1  # max delay before canceling queries        when reading WAL from archive;                 # -1 allows indefinite delay
**max_standby_streaming_delay** = -1        # max delay before canceling queries     when reading streaming WAL;              # -1 allows indefinite delay
#**wal_receiver_status_interval** = 10s     # send replies at least this often    # 0 disables
#**hot_standby_feedback** = on              # send info from standby to prevent     query conflicts

#### 复现步骤

1、备库提交查询t1表（不要退出该会话）

postgres=# begin;
BEGIN
postgres=# select * from dev;
(0 rows)

2、主库truncate表

postgres=# truncate table dev;
TRUNCATE TABLE
postgres=#

3、备库查看startup进程状态（出现waiting标识）

[root@node2 ~]# ps -ef|grep postgres
root       7206   7189  0 04:28 pts/0    00:00:00 su - postgres
postgres   7207   7206  0 04:28 pts/0    00:00:00 -bash
root       7303   7286  0 05:01 pts/1    00:00:00 su - postgres
postgres   7304   7303  0 05:01 pts/1    00:00:00 -bash
postgres   7407      1  0 05:08 pts/1    00:00:00 /home/postgres/pg96/bin/postgres
postgres   7408   7407  0 05:08 ?        00:00:00 postgres: logger process
**postgres   7409   7407  0 05:08 ?        00:00:00 postgres: startup process   recovering 00000006000000010000000E waiting waiting**
postgres   7410   7407  0 05:08 ?        00:00:00 postgres: checkpointer process
postgres   7411   7407  0 05:08 ?        00:00:00 postgres: writer process
postgres   7412   7407  0 05:08 ?        00:00:00 postgres: stats collector process
postgres   7413   7407  0 05:08 ?        00:00:00 postgres: wal receiver process   streaming 1/E02B4C8
postgres   7474   7304  0 05:14 pts/1    00:00:00 psql
postgres   7475   7407  0 05:14 ?        00:00:00 postgres: postgres postgres [local] idle in transaction
root       7477   7344  0 05:14 pts/2    00:00:00 grep --color=auto postgres

4、查看进程状态

[root@node2 ~]# pstack 7409
#0  0x00007ff879a04f53 in __select_nocancel () from /lib64/libc.so.6
#1  0x00000000009d61f6 in pg_usleep (microsec=1000000) at pgsleep.c:56
#2  0x00000000007fed49 in **WaitExceedsMaxStandbyDelay** () at standby.c:200
#3  0x00000000007fee85 in ResolveRecoveryConflictWithVirtualXIDs (waitlist=0x17fa2c8, reason=PROCSIG_RECOVERY_CONFLICT_LOCK) at standby.c:261
#4  0x00000000007ff07b in ResolveRecoveryConflictWithLock (locktag=...) at standby.c:404
#5  0x000000000080c4cc in ProcSleep (locallock=0x17f43c0, lockMethodTable=0xb5e120 <default_lockmethod>) at proc.c:1223
#6  0x00000000008056ed in WaitOnLock (locallock=0x17f43c0, owner=0x0) at lock.c:1745
#7  0x00000000008043b8 in LockAcquireExtended (locktag=0x7ffe39776240, lockmode=8, sessionLock=1 '\001', dontWait=0 '\000', reportMemoryError=1 '\001', locallockp=0x0) at lock.c:1026
#8  0x000000000080395d in LockAcquire (locktag=0x7ffe39776240, lockmode=8, sessionLock=1 '\001', dontWait=0 '\000') at lock.c:689
#9  0x00000000007ff40c in StandbyAcquireAccessExclusiveLock (xid=1859, dbOid=13325, relOid=17067) at standby.c:667
#10 0x00000000007ff8a7 in **standby_redo** (record=0x17f6950) at standby.c:822
#11 0x00000000005339b0 in StartupXLOG () at xlog.c:6989
#12 0x00000000007999ae in StartupProcessMain () at startup.c:210
#13 0x0000000000547528 in AuxiliaryProcessMain (argc=2, argv=0x7ffe39776e40) at bootstrap.c:419
#14 0x000000000079896a in StartChildProcess (type=StartupProcess) at postmaster.c:5306
#15 0x00000000007934ec in PostmasterMain (argc=1, argv=0x17caa40) at postmaster.c:1322
#16 0x00000000006daeae in main (argc=1, argv=0x17caa40) at main.c:228

发现startup进程在执行standby_redo时发生等待。等待时间受**max_standby_streaming_delay** 参数控制。



本质上上面两种情况都属于锁冲突的情况。

备库dev表持锁情况：

postgres=# select oid,* from pg_class where relname='dev';
  oid  | relname | relnamespace | reltype | reloftype | relowner | relam | relfilenode | reltablespace | relpages | reltuples | relallvi
sible | reltoastrelid | relhasindex | relisshared | relpersistence | relkind | relnatts | relchecks | relhasoids | relhaspkey | relhasru
les | relhastriggers | relhassubclass | relrowsecurity | relforcerowsecurity | relispopulated | relreplident | relfrozenxid | relminmxid
 | relacl | reloptions
-------+---------+--------------+---------+-----------+----------+-------+-------------+---------------+----------+-----------+---------
------+---------------+-------------+-------------+----------------+---------+----------+-----------+------------+------------+---------
----+----------------+----------------+----------------+---------------------+----------------+--------------+--------------+-----------
-+--------+------------
 17067 | dev     |         2200 |   17069 |         0 |       10 |     0 |       25275 |             0 |        0 |         0 |
    0 |             0 | f           | f           | p              | r       |        1 |         0 | f          | f          | f
    | f              | f              | f              | f                   | t              | d            |         1861 |          1
 |        |
(1 row)

postgres=# select relation,mode,granted from pg_catalog.pg_locks where relation=17067;
 relation |        mode         | granted
----------+---------------------+---------
    17067 | AccessExclusiveLock | f
    17067 | AccessShareLock     | t
(2 rows)

主库dev表持锁情况：

postgres=# select relation,mode,granted from pg_catalog.pg_locks where relation=17067;
 relation |        mode         | granted
----------+---------------------+---------
    17067 | ShareLock           | t
    17067 | AccessExclusiveLock | t
(2 rows)

主库truncate持有的ShareLock 、AccessExclusiveLock 阻塞了备库请求的AccessExclusiveLock 锁。发生了冲突。



#### 解决查询冲突

从库配置更改如下：

```
# - Standby Servers -
# These settings are ignored on a master server.
hot_standby = on                        # "on" allows queries during recovery
                                        # (change requires restart)
max_standby_archive_delay = 30s # max delay before canceling queries
#max_standby_archive_delay = -1 # max delay before canceling queries
​```                                    # when reading WAL from archive;
                                    # -1 allows indefinite delay
​```
max_standby_streaming_delay = 30s       # max delay before canceling queries
#max_standby_streaming_delay = -1       # max delay before canceling queries
```

修改完执行pg_ctl reload -D $PGDATA，使配置生效。

从库操作如下：

> ```
> [postgres@node2 ~]$ psql
> psql (9.6.13)
> Type "help" for help.
> postgres=# begin;
> BEGIN
> postgres=# select * from dev;
> ##  id
> (0 rows)
> postgres=#
> postgres=#
> postgres=# select * from dev;
> FATAL:  terminating connection due to conflict with recovery
> DETAIL:  User was holding a relation lock for too long.
> HINT:  In a moment you should be able to reconnect to the database and repeat your command.
> server closed the connection unexpectedly
>         This probably means the server terminated abnormally
>         before or while processing the request.
> The connection to the server was lost. Attempting reset: Succeeded.
> postgres=#
> ```

主库操作如下:

```
[postgres@node3 ~]$ psql
psql (9.6.13)
Type "help" for help.

postgres=# truncate dev;
TRUNCATE TABLE
```

发现等待大概30s左右，从库提交的那条查询被终止。报出下述错误：

```
FATAL:  terminating connection due to conflict with recovery
DETAIL:  User was holding a relation lock for too long.
```



## 查询冲突解决方法

[file:///C:/Users/cc/Documents/WeChat%20Files/caochao915156896/FileStorage/File/2019-09/PostgreSQL%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%20%E5%A4%87%E5%BA%93%E6%9F%A5%E8%AF%A2%E5%86%B2%E7%AA%81%20-%20User%20was%20holding%20shared%20buffer%20pin%20for%20too%20long-%E4%BA%91%E6%A0%96%E7%A4%BE%E5%8C%BA.mhtml](file:///C:/Users/cc/Documents/WeChat Files/caochao915156896/FileStorage/File/2019-09/PostgreSQL源码分析 备库查询冲突 - User was holding shared buffer pin for too long-云栖社区.mhtml)

#### 参考网址

https://yq.aliyun.com/articles/14644?spm=a2c4e.11153940.0.0.4b575f3cjjpQja